# ğŸ§  Text Prediction Using LSTM

This repository contains a deep learning project that uses an LSTM (Long Short-Term Memory) neural network to perform next-word prediction based on a given text corpus. The model is trained on the writings of Benjamin Franklin and can generate text sequences when given a seed phrase.

---

## ğŸ“Œ Objective

The primary objective of this project is to:

- Learn patterns and semantics from historical English text.
- Predict the most likely next word given a sequence of previous words.
- Generate new, coherent text based on a user-provided input phrase.

This project serves as an educational example of natural language processing (NLP), sequence modeling, and deep learning for text generation using TensorFlow and Keras.

---

## ğŸ› ï¸ Model Architecture

The model is a simple Sequential neural network built with Keras. It consists of:

- **Embedding Layer**: Converts words into dense vector representations.
- **LSTM Layer**: Learns dependencies between sequential words.
- **Dense Output Layer**: Applies a softmax activation to predict the next word.

```python
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_len-1))
model.add(LSTM(units=100))
model.add(Dense(units=vocab_size, activation='softmax'))

## ğŸ“š Dataset Details

- **File**: `franklin.txt`
- **Content**: Likely consists of public domain writings or speeches by Benjamin Franklin.

### ğŸ§¼ Preprocessing Steps:
- Removal of newlines, special characters (like quotation marks), and duplicate spaces.
- Tokenization using Keras `Tokenizer` (at the word level).
- Generation of input-output sequences for supervised training.
- Categorical one-hot encoding of output labels to be used in classification.

---

## ğŸ” Sequence Generation Process

The model is trained on short sequences of words, where the objective is to predict the next word given a sequence of prior words. A sliding window technique is used to create these input-output pairs.

